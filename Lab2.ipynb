{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbTUIJVrAaIr"
   },
   "source": [
    "<center><img src=\"http://what-when-how.com/wp-content/uploads/2012/07/tmp26dc139_thumb.png\" width=50% > </center>\n",
    "\n",
    "# <center> Assignment 2: Neighborhood Processing & Filters </center>\n",
    "<center> Computer Vision 1 University of Amsterdam </center>\n",
    "    <center> Due 23:59PM, September 24, 2022 (Amsterdam time) </center>\n",
    "    \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufYIQhgROv4X"
   },
   "source": [
    "##(100 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMW9vG1dBifa"
   },
   "source": [
    "## General guidelines\n",
    "Your code and discussion must be completed in this **jupyter notebook** before the deadline by submitting it to the Canvas Lab 2 Assignment. Submit your assignment in a **zip file**, with all the relevant files and images need to run your notebook. Name your zip file as follows:  **StudentID1_StudentID2_StudentID3.zip**\n",
    "For full credit, make sure you follow these guidelines:\n",
    "\n",
    "- Make sure you use the provided python environment. You can create the environment using conda and the provided YAML file using the following command: `conda env create --file=CV1_env.yaml`, then activate it as `conda activate cv1`. Using different packages versions may result in the impossibility to run the submitted code and therefore in the subtraction of points. Below you will find a code cell to check the versions of your packages. \n",
    "- Please express your thoughts concisely. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Answer all given questions.\n",
    "- Try to understand the problem as much as you can. When answering a question, give evidences (qualitative and/or quantitative results, references to papers, figures etc.) to support your arguments. Note that not everything might be explicitly asked for and you are expected to think about what might strengthen you arguments and make your notebook self-contained and complete.\n",
    "- Analyze your results and discuss them, e.g. why algorithm A works better than algorithm B in a certain problem.\n",
    "- Tables and figures must be accompanied by a brief description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.\n",
    "- Make sure all the code in your notebook runs without errors or bugs before submitting. Code that does not run can result in a lower grade. \n",
    "\n",
    "Late submissions are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAs‚Äô system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n",
    "Plagiarism note: Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBCvx1UiTn_j"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "error",
     "timestamp": 1663316103937,
     "user": {
      "displayName": "Pieter Pierrot",
      "userId": "05479018623794835299"
     },
     "user_tz": -120
    },
    "id": "DXbuUaRUud2H",
    "outputId": "d644980b-991a-4048-c408-a6827525b962"
   },
   "outputs": [],
   "source": [
    "# Make sure you're using the provided environment!\n",
    "assert cv2.__version__ == \"3.4.2\", \"You're not using the provided Python environment!\"\n",
    "assert np.__version__ == \"1.19.5\", \"You're not using the provided Python environment!\"\n",
    "assert matplotlib.__version__ == \"3.3.4\", \"You're not using the provided Python environment!\"\n",
    "assert sklearn.__version__ == \"0.23.0\"\n",
    "\n",
    "# Proceed to the next cell only if you don't get any error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIIOhzlprt6C"
   },
   "source": [
    "# 1 Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NbweJbQCUny"
   },
   "source": [
    "\n",
    "In this assignment, you will get familiar with fundamentals of neighborhood processing for image processing. These techniques allow for low-level image understanding via extraction of structural patterns such as edges and blobs. Similarly, they find an extensive use in image denoising and higher level image reasoning such as shape recognition. Moreover, neighborhood or block processing is one of the key components of *Convolutional Neural Networks*. Therefore, a good understanding of these\n",
    "procedures will be a stepping stone towards understanding more complex machinery used in computer vision and machine learning.\n",
    "\n",
    "In subsequent sections of this assignment, we will first explain neighborhood processing and introduce low-level filters commonly used to analyze images. After that, we will see how these mathematical concepts relate to practice by working through fundamental tasks such as denoising and segmentation. By the end of this assignment, you will have an overall understanding of the following:\n",
    "* Gaussian and Gabor filters\n",
    "* Edge detection and image denoising\n",
    "* Texture-based image segmentation\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "First we need two helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1GK_-R4cWPb"
   },
   "outputs": [],
   "source": [
    "### Use this for the last exercise\n",
    "\n",
    "def load_image(image_id: str='Polar'):\n",
    "  '''\n",
    "  Loads an image, resizes image with proper resize factor and sets proper color representation\n",
    "  :param image_id: id of an image: Kobi, Polar, Robin-1, Robin-2, Cows, SciencePark\n",
    "  :return: image\n",
    "  '''\n",
    "  if image_id == 'Kobi':\n",
    "    img = cv2.imread('./sample_data/sample_data/kobi.png')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    resize_factor = 0.25\n",
    "  elif image_id == 'Polar':\n",
    "    img = cv2.imread('./sample_data/sample_data/polar-bear-hiding.jpg')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    resize_factor = 0.75\n",
    "  elif image_id == 'Robin-1':\n",
    "    img = cv2.imread('./sample_data/sample_data/robin-1.jpg')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    resize_factor = 1\n",
    "  elif image_id == 'Robin-2':\n",
    "    img = cv2.imread('./sample_data/sample_data/robin-2.jpg')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    resize_factor = 0.5\n",
    "  elif image_id == 'Cows':\n",
    "    img = cv2.imread('./sample_data/sample_data/cows.jpg')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    resize_factor = 0.5\n",
    "  elif image_id == 'SciencePark':\n",
    "    img = cv2.imread('./sample_data/sample_data/sciencepark.jpg')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    resize_factor = 0.2\n",
    "  else:\n",
    "    raise ValueError('Image not available.')\n",
    "  img = cv2.resize(img, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-4o3ljKeJku"
   },
   "outputs": [],
   "source": [
    "def show_image(image, image_title:str= \"Polar\", cmap='gray'):\n",
    "  '''Displays image in grey scale'''\n",
    "  plt.figure()\n",
    "  plt.title(image_title)\n",
    "  plt.imshow(image, cmap=cmap)\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fk8dblKYr2z4"
   },
   "source": [
    "# 2 Neighborhood Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wI0keTjXDhS0"
   },
   "source": [
    "\n",
    "Neighborhood processing is simply about looking around a point $I(x, y)$ (i.e. pixel) in the image, $I$, and applying a function, $h(k, l)$, which measures certain properties or relationships between the pixels in that localized window. The function, $h(k, l)$, is generally referred to as the neighborhood operator or local operator. One of the most common forms of a neighborhood operator is a linear filter. Linear filters simply compute the weighted sum of neighboring pixel intensities and assign it to the pixel of interest (output $I_{out}(i, j)$). The filters in which we are interested here are usually represented as a square matrix.\n",
    "\n",
    "---\n",
    "**Hint**: Filters, kernels, weight matrices or masks are interchangeably used in the literature. A kernel is a matrix with which we describe a neighborhood operation. This operation can, for example, be edge detection or smoothing.\n",
    "\n",
    "---\n",
    "\n",
    "Linear filters are shifted over the entire image plane via operators such as correlation ($\\otimes$) and convolution ($\\ast$). Both of these operators are *linear shift-invariant* (LSI) implying that the filters behave the same way over the entire image. Discrete forms of these operators are given in the following:\n",
    "\n",
    "\n",
    "Correlation (1):\n",
    "<center>\n",
    "$\\mathbf{I}_{out} = I \\otimes  \\mathbf{h}\\\\\n",
    " \\mathbf{I}_{out}(i,j) = \\sum_{k,l}  \\mathbf{I}(i+k,j+l) \\mathbf{h}(k,l)$\n",
    "</center>\n",
    "Convolution (2):\n",
    "<center>\n",
    "$    \\mathbf{I}_{out} = \\mathbf{I} \\ast  \\mathbf{h}\\\\\n",
    " \\mathbf{I}_{out}(i,j) = \\sum_{k,l} \\mathbf{I}(i-k,j-l) \\mathbf{h}(k,l)$\n",
    " </center>\n",
    "\n",
    "\n",
    "---\n",
    "The following example illustrates the overall idea of neighborhood processing. The kernel or the mask convolves over the input image. In the case of linear filters, this is simply multiplying each pixel intensity with the corresponding weight in the kernel (see the yellowish $7x7$ window where the kernel is placed). In the example, the kernel is $7x7$ averaging mask. You can see its effect by comparing the red (before filtering) and the green (after filtering) frames.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hQY3W1wLmHv"
   },
   "source": [
    "**Example:**\n",
    "![](https://drive.google.com/uc?export=view&id=1HXt-WTi2Mg-jHYU4tvcKl4R29gi-g1QY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mZ_fs1wWRl8"
   },
   "source": [
    "### Question (10 pts)\n",
    "1.   What is the difference between correlation and convolution operators? How do they treat the signals $\\mathbf{I}$ and $\\mathbf{h}$?\n",
    "2.   Correlation and convolution operators are equivalent when we make an assumption on the form of the mask $\\mathbf{h}$. Can you identify the case?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylaRzZZNLPpU"
   },
   "source": [
    "1. Convolution and correlation are in essence the same operators, tranforming every element of the image through applying a matrix equation to a neighbourhood area of a pixel. The difference is that in the componentwise notation correlation is done by (ùëñ+ùëò,ùëó+ùëô), while convolution is computed through (ùëñ-ùëò,ùëó-ùëô). In other words, they are the same but with a transposed matrix.\n",
    "2. Correlation is convolution with a transposed matrix. They are the same if the mask h is symmetric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJlTkHdTOROo"
   },
   "source": [
    "# 3 Low-level filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Blbzx35sVsf"
   },
   "source": [
    "In this section, you will design common linear filters used in neighborhood processing. We will focus in particular on Gaussian and Gabor filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NhWPur7P1_Q"
   },
   "source": [
    "## 3.1 Gaussian Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oBHyce1saAI"
   },
   "source": [
    "### 3.1.1 1D Gaussian Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyGUgGekPz-a"
   },
   "source": [
    "\n",
    "\n",
    "The 1D  Gaussian filter is defined as follows:\n",
    "<center>\n",
    "$ G_{\\sigma}(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\text{ exp}(-\\frac{x^2}{2\\sigma^2}),$\n",
    "</center>\n",
    "where $\\sigma$ is the variance of the Gaussian. However, such formulation creates an infinitely large convolution kernel. In practice, the kernel is truncated with a `kernel_size` parameter such that $-\\left\\lfloor \\frac{kernel\\_size}{2}\\right\\rfloor \\leq x \\leq \\left\\lfloor \\frac{kernel\\_size}{2} \\right\\rfloor$, where $\\left\\lfloor . \\right\\rfloor$ is the floor operator. As an example, if `kernel_size` equals 3, $x \\in \\{ -1, 0, 1 \\} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2NtMMr9RSVv"
   },
   "source": [
    "### Exercise \n",
    "Now, implement the following *gauss1D* function.\n",
    "\n",
    "**Hint:** \n",
    "Do not forget to normalize your filter.\n",
    "\n",
    "**Note:** You are not allowed to use a Python built-in function provided by *SciPy* or other libraries to compute the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss1D(sigma, kernel_size):\n",
    "    G = np.zeros((1, kernel_size))\n",
    "    if kernel_size % 2 == 0:\n",
    "        raise ValueError('kernel_size must be odd, otherwise the filter will not have a center to convolve on')\n",
    "    for idx in range(kernel_size):\n",
    "        kernel_size_half = (kernel_size-1)/2\n",
    "        G[0,idx] = (1/(sigma*(2*3.141592653589793238)**1/2))*2.7182818284**(-((idx-kernel_size_half)**2)/(2*sigma**2))\n",
    "    G/=G.sum()\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss1D(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUwwgpFaR5XH"
   },
   "outputs": [],
   "source": [
    "# Run this to test your function:\n",
    "assert np.all(np.round(gauss1D(2,5), 4) == [0.1525, 0.2218, 0.2514, 0.2218, 0.1525]) # check if values are close enough if assert gives error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkT7BklaTUmj"
   },
   "source": [
    "### 3.1.2 2D Gaussian Filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cW9S9S-CspJn"
   },
   "source": [
    "\n",
    "One of the most important properties of 2D Gaussian kernels is separability. Therefore, convolving an image with a 2D Gaussian is equivalent to convolving\n",
    "the image twice with a 1D Gaussian filter, once along the x-axis and once along the y-axis **separately**. A 2D Gaussian kernel can then be defined as a product of two 1D Gaussian kernels:\n",
    "<center>\n",
    "$\n",
    "G_{\\sigma}(x, y) = G_{\\sigma}(x) \\times G_{\\sigma}(y)$ *(Eq. A)*\n",
    "\n",
    "$\n",
    " = \\frac{1}{\\sigma^2 2\\pi}\\text{ exp}(-\\frac{x^2 + y^2}{2\\sigma^2})\n",
    " $ *(Eq. B)*\n",
    " </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "762bvpRYTual"
   },
   "source": [
    "### Exercise \n",
    "\n",
    "Implement `gauss2D` function that corresponds to *Eq. A* (not *Eq. B*) and you should make use of `gauss1D`.\n",
    "\n",
    "**Note:** Again, you are not allowed to use a Python built-in function provided by *SciPy* or other libraries to compute the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhfGMWffTtzm",
    "outputId": "62a98d7b-a3d5-4ada-b4c2-ef47982a4dea"
   },
   "outputs": [],
   "source": [
    "def gauss2D(sigma_x, sigma_y, kernel_size):\n",
    "    G = np.zeros((kernel_size, kernel_size))\n",
    "    if kernel_size % 2 == 0:\n",
    "        raise ValueError('kernel_size must be odd, otherwise the filter will not have a center to convolve on')\n",
    "    kernel_size_half = (kernel_size-1)/2\n",
    "    for idx in range(kernel_size):\n",
    "        G[idx] = gauss1D(sigma_x, kernel_size)\n",
    "    y_values = gauss1D(sigma_y, kernel_size)\n",
    "    for idx in range(kernel_size):\n",
    "        G[idx] *= y_values[0,idx]        \n",
    "    return G             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgMHxPlgVbgQ"
   },
   "outputs": [],
   "source": [
    "# Run this to test your function:\n",
    "assert np.all( np.round(gauss2D(2, 2, 3) , 4) == [[0.1019, 0.1154, 0.1019],\n",
    "       [0.1154, 0.1308, 0.1154],\n",
    "       [0.1019, 0.1154, 0.1019]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2ZB6tEeV_Ls"
   },
   "source": [
    "###Question (5pts)\n",
    "What is the difference between convolving an image with (1) a 2D Gaussian kernel and (2) a 1D Gaussian kernel in the x- and y-direction? Will the result be the same? What is their computational complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gTOdwi4WCyi"
   },
   "source": [
    "Yes, the image will be sthe same, if the first the whole image is convolved with a 1D filter and then that image will be convolved with a 1D filter along the other axis. The computational complexity will be less: Consider a 5x5 image that is to be convolved with a 3x3 2D gaussian. If done with two filters, it will be 2times\\*25pixels\\*3multiplications (150) while if done with a 2D filter, it would be 1time\\*25pixels\\*9multiplications (225)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViVNlHPOa2Jd"
   },
   "source": [
    "### 3.1.3 Gaussian Derivatives\n",
    "So far the Gaussian kernels that we computed are mainly targeted to image enhancement algorithms (e.g. denoising an image). These kernels can also be used for detecting changes in the image intensity pixels. These low-level features can then further be used as building blocks for more complicated tasks like object detection or segmentation.\n",
    "\n",
    "\n",
    "Concretely, the  first order derivative of the 1D Gaussian kernel is given by:\n",
    "\n",
    "<center>\n",
    "$\n",
    "\\frac{d}{dx}G_\\sigma(x)  =\\frac{d}{dx}\\left( \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp(-\\frac{x^2}{2\\sigma^2}) \\right )$\n",
    "\n",
    "$\n",
    " = -\\frac{x}{\\sigma^3\\sqrt{2\\pi}}\\exp(-\\frac{x^2}{2\\sigma^2})$ \n",
    "*(Eq. C)*\n",
    "\n",
    " $\n",
    " = -\\frac{x}{\\sigma^2}G_\\sigma(x)\n",
    " $\n",
    "</center>\n",
    "Similarly, the first order derivative of the 2D Gaussian kernel can be obtained by computing $\\frac{d}{dx}G_\\sigma(x,y)$ and $\\frac{d}{dy}G_\\sigma(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMgRkSaSa-7u"
   },
   "source": [
    "#### Question (5pts)\n",
    "A second order derivative of the Gaussian kernel can also be computed. Why\n",
    "is it interesting to design a second order kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtsxpaP8bGqC"
   },
   "source": [
    "**Answer**: A second order will highlight the areas of the image of high intensity change, this can be used to detect edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtMu-uUgbIHi"
   },
   "source": [
    "## 3.2 Gabor filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G40RQGvgsxAj"
   },
   "source": [
    "Gabor filters fall into the category of linear filters and are widely used for *texture analysis*. The reason why they are a good choice for texture analysis is that they localize well in the frequency spectrum (*optimally* bandlimited) and therefore work as flexible *band-pass* filters.\n",
    "\n",
    "In the following image you can see even (cosine-modulated) and odd parts (sine-modulated) of Gabor filters with fixed-œÉ Gaussian. You can observe time-domain filters for the modulating sinusoidals of central frequencies, 10, 20, 30, 40 and 50 Hz, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qs5KY3RIL23Z"
   },
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1wv6ZUOfiHMDgg0jW7n5lqkaFiFztjnEo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPyZ-8JveugN"
   },
   "source": [
    "Gabor filters with varying center frequencies are sensitive to different\n",
    "frequency bands. Notice that the neighboring (in the frequency spectrum) filters minimally interfere with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLgAHvd3MBxU"
   },
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1hUQaKE_TwC-9_jMIao1CCxyjembMrUab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1mP6EytbZE2"
   },
   "source": [
    "### 3.2.1 1D Gabor Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0qj4afHtBkK"
   },
   "source": [
    "For the sake of simplicity, we start by studying what a Gabor function is using 1D signals (e.g. speech). The idea will later be generalized to the 2D case, which is suited for our primary interest, images. A Gabor function is a Gaussian function modulated with a complex sinusoidal carrier signal. Let us denote the Gaussian with $x(t)$ and complex sinusoidal with $m(t)$. Then, a Gabor function $g(t)$ can be formulated by:\n",
    "\n",
    "$\n",
    "g(t) = x(t) m(t)   \n",
    "$ *(Eq. D)*\n",
    "\n",
    "where $x(t) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{t^2}{2\\sigma^2}}$ and $m(t)=e^{j 2 \\pi f_c t} = e^{j w_c t}$. $\\sigma$ is the parameter determining the spread of the Gaussian and $w_c$ is the central frequency of the carrier signal.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "A complex sinusoidal can be represented as follows using the *Euler's formula*:\n",
    "$e^{jwt} = \\cos(wt) + j\\sin(wt)$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Using Euler's formula, we get the following:\n",
    "\\begin{align}\n",
    "    g(t) &= x(t)m(t) \\\\\n",
    "    g(t) &= \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{t^2}{2\\sigma^2}} e^{j w_c t} \\\\\n",
    "    g(t) &= \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{t^2}{2\\sigma^2}} [\\cos(w_c t) + j\\sin(w_c t)]\n",
    "\\end{align}\n",
    "We can further arrange the terms and arrive at the following form\n",
    "\\begin{align}\n",
    "    g(t) = g_e(t) + jg_o(t)\n",
    "\\end{align}\n",
    "where $g_e(t)$ and $g_o(t)$ are the even and odd parts arranged orthogonally on the complex plane $\\mathbf{Z}^2$. In practice, one can use either the even or the odd part for filtering purposes (or one can use the complex form).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2fL29EAbaYt"
   },
   "source": [
    "### 3.2.2 2D Gabor Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Wor-yxqtLnl"
   },
   "source": [
    "\n",
    "The Gabor filters can also be defined in 2D as well. The main difference lies in the dimensionality of the signals (i.e. carrier and gaussian). A sine wave in 2D is described by two orthogonal spatial frequencies $u_0$ and $v_0$ such that it is given as $s(x,y) = sin(2\\pi(u_0 x + v_0 y))$ whereas a 2D gaussian is simply $C e^{-(\\frac{(x-x_0)^2}{2\\sigma_x^2}) + \\frac{(y-y_0)^2}{2\\sigma_y^2})}$ with $C$ being a normalizing constant. 2D Gabor function then takes the following forms in the real and complex parts:\n",
    "\n",
    "\\begin{align}\n",
    "    g_{real}(x,y; \\lambda, \\theta, \\psi, \\sigma, \\gamma) = \\exp\\left(-\\frac{x^{\\prime2}+\\gamma^2 y^{\\prime2}}{2\\sigma^2}\\right)\n",
    "\\cos\\left( 2\\pi \\frac{x^{\\prime}}{\\lambda} + \\psi  \\right)                                              \n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "g_{im}(x,y; \\lambda, \\theta, \\psi, \\sigma, \\gamma) &= \\exp\\left(-\\frac{x^{\\prime2}+\\gamma^2 y^{\\prime2}}{2\\sigma^2}\\right)\n",
    "\\sin\\left( 2\\pi \\frac{x^{\\prime}}{\\lambda} + \\psi  \\right)\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\\begin{align}\n",
    "    x^\\prime &= x\\cos\\theta + y\\sin\\theta \\\\\n",
    "    y^\\prime &= -x\\sin\\theta + y\\cos\\theta\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32eQTHQobdGG"
   },
   "source": [
    "####Question 4 (5pts)\n",
    "Conduct a self-study on the Gabor filters. Explain shortly what the parameters $\\lambda, \\theta, \\psi, \\sigma, \\gamma$ control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfCvFPCltWXs"
   },
   "source": [
    "**Answer**: \n",
    "* lambda is the specified wavelength of the sinusoidal part of the equation\n",
    "* theta denotes the degree of rotation of the parallel waves in the 2D setting. It can go from 0 to 2pi.\n",
    "* Psy is the phase offset of the sinusoidal function. \n",
    "* Sigma is the standard deviation of the Gaussian envelope. \n",
    "* Gamma is a variable that controls the aspect ratio of the Gaussian envelope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIpIVPE4tvE9"
   },
   "source": [
    "###Exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QVZG7MciTJn"
   },
   "source": [
    "##### Design array of Gabor Filters\n",
    "\n",
    "Now, you will create a Gabor Filterbank. A filterbank is a collection of filters with varying properties (e.g. {shape, texture}). A Gabor filterbank consists of Gabor filters of distinct orientations and scales. We will use this bank to extract texture information from the input image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTqtQqRqvm6E"
   },
   "source": [
    "Your task is to implement function `createGabor` but to do that you will need some helper functions, which are defined below. Finish the implementation of those and then use them in `createGabor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeM6YS5blFZ-"
   },
   "outputs": [],
   "source": [
    "def generateRotationMatrix(theta):\n",
    "  # Returns the rotation matrix. \n",
    "  # Hint: https://en.wikipedia.org/wiki/Rotation_matrix\n",
    "  \n",
    "  rotMat = [[np.cos(theta),np.sin(theta)],[-np.sin(theta),np.cos(theta)]] \n",
    "  # TODO: code the rotation matrix which fits gabor equation given theta. \n",
    "  return rotMat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCXnvzLav1Xg"
   },
   "outputs": [],
   "source": [
    "def createCos(rot_x, lamda, psi):\n",
    "  # Returns the 2D cosine carrier. \n",
    "\n",
    "    cosCarrier = np.cos(2*np.pi*rot_x/lamda+psi)\n",
    "  # TODO: Implement the cosine given rot_x, lamda and psi.\n",
    "  # Reshape the vector representation to matrix.\n",
    "    cosCarrier = np.reshape(cosCarrier, (np.int32(np.sqrt(len(cosCarrier))), -1))\n",
    "    return cosCarrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGPBzDW_v1c8"
   },
   "outputs": [],
   "source": [
    "def createSin(rot_x, lamda, psi):\n",
    "  # Returns the 2D sine carrier. \n",
    "  \n",
    "    sinCarrier = np.sin(2*np.pi*rot_x/lamda+psi)\n",
    "  # Reshape the vector representation to matrix.\n",
    "    sinCarrier = np.reshape(sinCarrier, (np.int32(np.sqrt(len(sinCarrier))), -1))\n",
    "    return sinCarrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZYsGehtwBUf"
   },
   "outputs": [],
   "source": [
    "def createGauss(rot_x, rot_y, gamma, sigma):\n",
    "  # Returns the 2D Gaussian Envelope. \n",
    "    #gaussEnv = gauss2D(sigma_x, sigma_y*gamma, kernel_size)\n",
    "    gaussEnv = np.exp(-(rot_x**2+gamma**2*rot_y**2)/2*sigma**2)\n",
    "    gaussEnv /= gaussEnv.sum()\n",
    "    gaussEnv = np.reshape(gaussEnv, (np.int32(np.sqrt(len(gaussEnv))), -1))\n",
    "    return gaussEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCwJtg9fbgUz"
   },
   "source": [
    "Implement the function `createGabor` using above helper functions and equations for $g_{real} $ and $g_{im}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lsf82AdMlbOO"
   },
   "outputs": [],
   "source": [
    "def createGabor(sigma, theta, lamda, psi, gamma):\n",
    "    '''\n",
    "    Creates a complex valued Gabor filter.\n",
    "    myGabor = createGabor(sigma, theta, lamda, psi, gamma) generates Gabor kernels. \n",
    "    :param sigma: Standard deviation of Gaussian envelope.\n",
    "    :param theta: Orientation of the Gaussian envelope. Takes arguments in the range [0, pi/2).\n",
    "    :param lamda: The wavelength for the carriers. The central frequency (w_c) of the carrier signals.\n",
    "    :param psi: Phase offset for the carrier signal, sin(w_c . t + psi).\n",
    "    :param gamma: Controls the aspect ratio of the Gaussian envelope\n",
    "    :return: myGabor - A matrix of size [h,w,2], holding the real and imaginary \n",
    "                        parts of the Gabor in myGabor(:,:,1) and myGabor(:,:,2), respectively.\n",
    "    '''\n",
    "\n",
    "    # Set the aspect ratio.\n",
    "    sigma_x = sigma\n",
    "    sigma_y = float(sigma)/gamma\n",
    "\n",
    "    # Generate a grid\n",
    "    nstds = 3\n",
    "    xmax = max(abs(nstds*sigma_x*np.cos(theta)),abs(nstds*sigma_y*np.sin(theta)))\n",
    "    xmax = np.ceil(max(1,xmax))\n",
    "    ymax = max(abs(nstds*sigma_x*np.sin(theta)),abs(nstds*sigma_y*np.cos(theta)))\n",
    "    ymax = np.ceil(max(1,ymax))\n",
    "\n",
    "    # Make sure that we get square filters. \n",
    "    xmax = max(xmax,ymax)\n",
    "    ymax = max(xmax,ymax)\n",
    "    xmin = -xmax \n",
    "    ymin = -ymax\n",
    "\n",
    "    # Generate a coordinate system in the range [xmin,xmax] and [ymin, ymax]. \n",
    "    [x,y] = np.meshgrid(np.arange(xmin, xmax+1), np.arange(ymin, ymax+1))\n",
    "\n",
    "    # Convert to a 2-by-N matrix where N is the number of pixels in the kernel.\n",
    "    XY = np.concatenate((x.reshape(1, -1), y.reshape(1, -1)), axis=0)\n",
    "\n",
    "    # Compute the rotation of pixels by theta.\n",
    "    # Hint: Create appropriate rotation matrix to compute the rotated pixel coordinates: rot(theta) * XY.\n",
    "    rotMat = generateRotationMatrix(theta)\n",
    "    #print(rotMat.shape, XY.shape)\n",
    "    rot_XY = np.matmul(rotMat, XY)\n",
    "    rot_x = rot_XY[0,:]\n",
    "    rot_y = rot_XY[1,:]\n",
    "\n",
    "\n",
    "    # Create the Gaussian envelope.\n",
    "    # IMPLEMENT the helper function createGauss above.\n",
    "    gaussianEnv = createGauss(rot_x, rot_y, gamma, sigma)\n",
    "\n",
    "    # Create the orthogonal carrier signals.\n",
    "    # IMPLEMENT the helper functions createCos and createSin above.\n",
    "    cosCarrier = createCos(rot_x, lamda, psi)\n",
    "    sinCarrier = createSin(rot_x, lamda, psi)\n",
    "\n",
    "    # Modulate (multiply) Gaussian envelope with the carriers to compute \n",
    "    # the real and imaginary components of the complex Gabor filter. \n",
    "    # TODO: modulate gaussianEnv with cosCarrier\n",
    "    myGabor_real = gaussianEnv*cosCarrier \n",
    "    # TODO: modulate gaussianEnv with sinCarrier\n",
    "    myGabor_imaginary = gaussianEnv*sinCarrier  \n",
    "\n",
    "    # Pack myGabor_real and myGabor_imaginary into myGabor.\n",
    "    h, w = myGabor_real.shape\n",
    "    myGabor = np.zeros((h, w, 2))\n",
    "    myGabor[:,:,0] = myGabor_real\n",
    "    myGabor[:,:,1] = myGabor_imaginary\n",
    "\n",
    "    # Uncomment below lines from \"fig = plt.figure()\" to see how are the gabor filters\n",
    "    #figure\n",
    "    #subplot(121), imshow(myGabor_real,[])\n",
    "    #subplot(122), imshow(myGabor_imaginary, [])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.imshow(myGabor_real)    # Real\n",
    "    ax.axis(\"off\")\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.imshow(myGabor_imaginary)    # Real\n",
    "    ax.axis(\"off\")\n",
    "    print(myGabor.shape)\n",
    "    return myGabor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#createGabor(1, 30, 20, 0, 0.1)\n",
    "gabor = createGabor(sigma=2, theta=13, lamda=5, psi=0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38iQckSqkLjC"
   },
   "source": [
    "#### Question (5pts)\n",
    "Visualize how the parameters $\\theta$, $\\sigma$ and $\\gamma$ affect the filter in spatial domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Increasing theta\n",
    "gabor = createGabor(sigma=1, theta=0, lamda=3, psi=0, gamma=1)\n",
    "gabor = createGabor(sigma=1, theta=0.5*np.pi, lamda=3, psi=0, gamma=1)\n",
    "gabor = createGabor(sigma=1, theta=0.75*np.pi, lamda=3, psi=0, gamma=1)\n",
    "gabor = createGabor(sigma=1, theta=2*np.pi, lamda=3, psi=0, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Increasing sigma\n",
    "gabor = createGabor(sigma=0.1, theta=0, lamda=5, psi=0, gamma=1)\n",
    "gabor = createGabor(sigma=0.5, theta=0, lamda=5, psi=0, gamma=1)\n",
    "gabor = createGabor(sigma=0.7, theta=0, lamda=5, psi=0, gamma=1)\n",
    "gabor = createGabor(sigma=1, theta=0, lamda=5, psi=0, gamma=1)\n",
    "gabor = createGabor(sigma=1.5, theta=0, lamda=5, psi=0, gamma=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Increasing gamma:\n",
    "gabor = createGabor(sigma=1, theta=0, lamda=5, psi=0, gamma=0.1)\n",
    "gabor = createGabor(sigma=1, theta=0, lamda=5, psi=0, gamma=0.5)\n",
    "gabor = createGabor(sigma=1, theta=0, lamda=5, psi=0, gamma=1)\n",
    "gabor = createGabor(sigma=1, theta=0, lamda=5, psi=0, gamma=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iv13duslkHKe"
   },
   "source": [
    "As can be seen, theta controls the rotation, sigma the variance, and gamma the aspect ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E41QZ9ZsO6vf"
   },
   "source": [
    "# 4 Applications in image processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXyH4WNDJtSK"
   },
   "source": [
    "## 4.1 Noise in digital images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0vlajJhKojZ"
   },
   "source": [
    "The quality of digital images can be affected in different ways. For example, the\n",
    "acquisition process can be very noisy and with a low-resolution (e.g. some medical\n",
    "imaging modalities only generate a 128x128 image). Noise can also come from the\n",
    "user who set wrong parameters on the digital camera. Consequently, different computer vision algorithms are required to enhance noisy or corrupted images. With the\n",
    "growing amount of photos taken every day, image enhancement has then become a\n",
    "very active area of research.\n",
    "\n",
    "In this section, we only focus on simple algorithms to correct noise coming typically from the sensor of your camera. Many other types of noise or corruption can happen but are out of the scope of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gZn2TPQKtA8"
   },
   "source": [
    "### 4.1.1 Salt-and-pepper noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_vRqsMUK3XV"
   },
   "source": [
    "Noise can also occur with over-exposition causing a ‚Äùhot‚Äù pixel or with a defective\n",
    "sensor causing a ‚Äùdead‚Äù pixel. This is called salt-and-pepper noise. Pixels in the\n",
    "image are randomly replaced by either a white or black pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcgoNmEWK7Zk"
   },
   "source": [
    "### 4.1.2 Additive Gaussian noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sQFRLM8LA2e"
   },
   "source": [
    "Noise also occurs frequently when the camera heats up. This is called thermal noise\n",
    "and this can be modeled as an additive Gaussian noise. Every pixel in the image\n",
    "has a noise component that corresponds to a random value chosen independently\n",
    "from the same Gaussian probability distribution. The Gaussian distribution has a\n",
    "mean of 0 and its standard deviation corresponds to a parameter.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{rl}\n",
    "\\mathbf{I}^{\\prime}(x) = \\mathbf{I}(x) + \\epsilon \\text{, where } \\epsilon \\sim  \\mathcal{N}(0, \\sigma^2)\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{I}^{\\prime}$ is the noisy image and $\\mathbf{I}$ is the original image without any noise $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8rYWOruVbNB"
   },
   "source": [
    "## 4.2 Image denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4WNOpLWVlnA"
   },
   "source": [
    "### 4.2.1 Quantitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI5c7v3zVsbD"
   },
   "source": [
    "The peak signal-to-noise ratio (PSNR) is a commonly used metric to quantitatively evaluate the performance of image enhancement algorithms. It is derived from the mean squared error (MSE):\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{rl}\n",
    "MSE = \\frac{1}{m \\cdot n}\\sum\\limits_{x,y}\\Big[\\mathbf{I}(x,y) - \\mathbf{\\hat{I}}(x,y)\\Big]^2\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "where $\\mathbf{I}$ is the original image of size $m\\times n$ and $\\mathbf{\\hat{I}}$ its approximation (i.e. in our case an enhanced corrupted image). The PSNR corresponds to:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "PSNR & = 10 \\cdot \\log_{10} \\Big(\\frac{\\mathbf{I}_{max}^2}{MSE}\\Big) \\\\\n",
    " & = 20 \\cdot \\log_{10} \\Big(\\frac{\\mathbf{I}_{max}}{\\sqrt{MSE}}\\Big) \\\\\n",
    " & = 20 \\cdot \\log_{10} \\Big(\\frac{\\mathbf{I}_{max}}{RMSE}\\Big)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where $\\mathbf{I}_{max}$ is the maximum pixel value of $\\mathbf{I}$ and RMSE is the root of the MSE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YIwJ8fSYgBu"
   },
   "source": [
    "###Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vw8ev7NXYpKj"
   },
   "source": [
    "Implement **myPSNR**\n",
    "\n",
    "**Note:** You are not allowed to use the Python built-in functions provided in *PIL* and *Skimage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzrSRfzrZVhd"
   },
   "outputs": [],
   "source": [
    "def myPSNR(orig_image, approx_image):\n",
    "    \n",
    "    \"\"\"\n",
    "    ================\n",
    "    Your code here\n",
    "    ================\n",
    "    \"\"\"\n",
    "    orig_image = orig_image.astype(np.float32)\n",
    "    approx_image = approx_image.astype(np.float32)\n",
    "    \n",
    "    se = np.sum((orig_image - approx_image)**2)\n",
    "    mse = se /(3*orig_image.shape[0] * orig_image.shape[1]) \n",
    "    PSNR = 20 * np.log10(orig_image.max()/ np.sqrt(mse))\n",
    "    return PSNR\n",
    "\n",
    "## Q2 ##\n",
    "salt_pepper = cv2.imread('sample_data/sample_data/image1_saltpepper.jpg')\n",
    "img1 = cv2.imread('sample_data/sample_data/image1.jpg')\n",
    "\n",
    "print('Q2: PSNR between image1_saltpepper.jpg and image1.jpg:')\n",
    "print(myPSNR(salt_pepper, img1))\n",
    "\n",
    "## Q3 ##\n",
    "img1_gauss = cv2.imread('sample_data/sample_data/image1_gaussian.jpg')\n",
    "print('\\n Q3: PSNR between image1_gaussian.jpg and image1.jpg:')\n",
    "print(myPSNR(img1_gauss, img1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6fuj5vMWw33"
   },
   "source": [
    "### Questions (10 pts)\n",
    "1.   Explain briefly in your own words what the PSNR is (without any equations). When comparing different methods with the PSNR metric, is a higher value the better or the opposite?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPB_GO6QbMzH"
   },
   "source": [
    "PSNR is an evaluation metric to quantitatively determine the quality of a reconstructed image with respect to the original image. For the PSNR, we divide by the MSE, so the lower the MSE, the higher the PSNR. So a high PSNR indicates a better reconstruction of the original. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hO26559KbNN-"
   },
   "source": [
    "2.   Using your implemented function **myPSNR**, compute the PSNR between image1_saltpepper.jpg and image1.jpg. What PSNR value did you get?\n",
    "\n",
    "  **Hint:**\n",
    "Make sure that dtype of image is float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rUxvvozbQSf"
   },
   "source": [
    "15.532209659519152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YS86lVJbNQh"
   },
   "source": [
    "3.   Using your implemented function **myPSNR**, compute the PSNR between image1_gaussian.jpg and image1.jpg. What PSNR value did you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIMB6x5-Ydbo"
   },
   "source": [
    "17.170831835645814"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-pj7HA9aqAq"
   },
   "source": [
    "### 4.2.2 Neighborhood processing for image denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lpNUDhxayLY"
   },
   "source": [
    "We will now design filters to remove these two types of noise. The function will denoise the image by either applying:\n",
    "\n",
    "1. *box filtering*: You can use **cv2.blur** function.\n",
    "2. *median filtering*: You can use **cv2.medianBlur** function.\n",
    "3. *Gaussian filtering*: You must use your **cv2.GaussianBlur** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_OMxltHbh2j"
   },
   "source": [
    "###Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhSp8tBubucj"
   },
   "source": [
    "Implement **denoise**\n",
    "\n",
    "**Hints:** \n",
    "1. **kernel_type** is just a string to specify the kernel type.\n",
    "2. ****kwargs** allows to have an undefined key-value pairs in a Python function. For example, you can have **sigma** and  **kernel_size** as argument when using a Gaussian kernel but only **kernel_size** when using a box kernel. For more information about how ****kwargs** works, take a look at [usage of kwargs](https://book.pythontips.com/en/latest/args_and_kwargs.html#usage-of-kwargs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASH65zVMdCsq"
   },
   "outputs": [],
   "source": [
    "def denoise(image, kernel_type, k = 9, sigma = 3):\n",
    "#     for key, value in kwargs.items():\n",
    "#         key = value\n",
    "        \n",
    "    if kernel_type == 'box':\n",
    "        imOut = cv2.blur(image, ksize = (k,k))\n",
    "    elif kernel_type == 'median':\n",
    "        imOut = cv2.medianBlur(image, ksize = k)\n",
    "    elif kernel_type == 'gaussian':\n",
    "        imOut = cv2.GaussianBlur(image, ksize=(k,k), sigmaX = sigma)\n",
    "    else:\n",
    "        print('Operation Not implemented')\n",
    "    return imOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isKllbE-d1id"
   },
   "source": [
    "### Question (20 pts)\n",
    "1.   Using your implemented function **denoise**, try denoising image1_saltpepper.jpg and image1_gaussian.jpg by applying the following filters:\n",
    "\n",
    "      (a) Box filtering of size: 3x3, 5x5, and 7x7.\n",
    "\n",
    "      (b) Median filtering with size: 3x3, 5x5 and 7x7.\n",
    "\n",
    "     Show the denoised images in the cell below. You can use tables to present your quantitative results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_pepper = cv2.imread('sample_data/sample_data/image1_saltpepper.jpg')\n",
    "\n",
    "for filt in ['box', 'median']:\n",
    "    for k in [3,5,7]:\n",
    "        imout = denoise(salt_pepper, filt, k)\n",
    "        print('denoised image1_saltpepper.jpg using the {}x{} {} filter \\n'.format(k,k,filt))\n",
    "        plt.imshow(imout)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_gauss = cv2.imread('sample_data/sample_data/image1_gaussian.jpg')\n",
    "\n",
    "\n",
    "for filt in ['box', 'median']:\n",
    "    for k in [3,5,7]:\n",
    "        imout = denoise(img1_gauss, filt, k)\n",
    "        print('denoised image1_gaussian.jpg using the {}x{} {} filter \\n'.format(k,k,filt))\n",
    "        plt.imshow(imout)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ih0Xb1WZcaAs"
   },
   "source": [
    "2.   Using your implemented function **myPSNR**, compute the PSNR for every denoised image (12 in total) wrt the original image. What is the effect of the filter size on the PSNR? Report the results (in a table) and discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_pepper = cv2.imread('sample_data/sample_data/image1_saltpepper.jpg')\n",
    "original_image = cv2.imread('sample_data/sample_data/image1.jpg')\n",
    "\n",
    "for filt in ['box', 'median']:\n",
    "    for k in [3,5,7]:\n",
    "        imout = denoise(salt_pepper, filt, k)\n",
    "        psnr = myPSNR(original_image, imout)\n",
    "        \n",
    "        print('PSNR of image1_saltpepper.jpg using the {}x{} {} filter wrt original image: {}\\n'.format(k,k,filt, psnr))\n",
    "\n",
    "img1_gauss = cv2.imread('sample_data/sample_data/image1_gaussian.jpg')\n",
    "for filt in ['box', 'median']:\n",
    "    for k in [3,5,7]:\n",
    "        imout = denoise(img1_gauss, filt, k)\n",
    "        psnr = myPSNR(original_image, imout)\n",
    "        \n",
    "        print('PSNR of image1_gaussian.jpg using the {}x{} {} filter wrt original image: {}\\n'.format(k,k,filt, psnr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXmdRw3Tcjct"
   },
   "source": [
    "For both the box filter we notice that an increase in kernel size from 3x3 to 5x5 leads to an increase in PSNR and a decrease when the kernel size goes from 5x5 to 7x7. So for the box filter 5x5 seems the best kernel size for denoising the two images. For the median filter the 3x3 appears to be the best size for denoising the two images. This is because the median filter uses the median pixel value of the kernel to replace the pixel value and as we increase the kernel size, the less likely it is that the median of the value is close to the original value of the pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UF2XuuyKcbcB"
   },
   "source": [
    "3.   Which is better for the salt-and-pepper noise, box or median filters? Why? What about the Gaussian noise?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjeTDaI_ckYH"
   },
   "source": [
    "For both salt-and-pepper noise as Gaussian noise we observe higher PSNR scores for the median filters, meaning they perform better. With salt and pepper noise some pixels will either turn black or white, so have a pixel value of either 0 or 1. When using a box filter we take the average of the pixel surrounding, and this black or white pixel will influence the average heavily as it differs greatly from the surrounding values. Therefore the new value of the black or white pixel will still deviate from the values of the surrounding values and also the original value of the pixel. The median filter, uses the median value of its surrounding pixels as the new value for the black or white pixel. The new pixel value will be closer to the original in this way than when using the box filter since the median value is used and the black or white pixel 'replaced' instead of averaged with its surrounding pixel values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoMfg5eccdAX"
   },
   "source": [
    "\n",
    "4.   Try denoising image1_gaussian.jpg using a Gaussian filtering. Choose an appropriate window size and standard deviation and justify your choice. Show the denoised images in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_gauss = cv2.imread('sample_data/sample_data/image1_gaussian.jpg')\n",
    "\n",
    "plt.imshow(denoise(img1_gauss, 'gaussian', 9, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBF3hOk-pnAA"
   },
   "source": [
    "We pick a kernel size of 9x9 and a standard deviation of 3. This causes the weights at the edges of the mask to approximate zero and so use the complete gaussian blob in the kernel, effectively removing the gaussian noise. The kernel size is set to 9 because this seems to remove the noise without blurring the image to much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbhpdDM9ceii"
   },
   "source": [
    "\n",
    "5.   What is the effect of the standard deviation on the PSNR? Report the results (in a table) and discuss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_gauss = cv2.imread('sample_data/sample_data/image1_gaussian.jpg')\n",
    "original_image = cv2.imread('sample_data/sample_data/image1.jpg')\n",
    "\n",
    "for sigma in np.linspace(0, 9, 10):\n",
    "    imout = denoise(img1_gauss, 'gaussian', k=9, sigma=sigma)\n",
    "    psnr = myPSNR(original_image, imout)\n",
    "    print('PSNR of gaussian filtered image(with sigma  = {}) w.r.t original image: {}'.format(sigma, psnr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDaSrQvXclwD"
   },
   "source": [
    "We observe a decline in PNSR when the standard deviation increases. So when the standard deviation of the gaussian filter increases, the quality of the reconstruction of the original images decreases according to the PNSR. This is because when the standard deviation of the gaussian filter increases, the images becomes more blurry resulting in a lower quality reconstruction and thus a lower PSNR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7ZZvTGxcfp5"
   },
   "source": [
    "\n",
    "6.   What is the difference among median filtering, box filtering and Gaussian filtering? Briefly explain how they are different at a conceptual level. If two filtering methods give a PSNR in the same ballpark, can you see a qualitative difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9xrXuV7cmS1"
   },
   "source": [
    "Median filtering assigns the median pixel value of the pixel values in the kernel to a pixel. Box filtering assigns the average pixel value of the pixel values in the kernel. Gaussian filtering applies a gaussian function to the kernel, which can be considered as a weighted box filter. The further away a pixel is from the center of the kernel, the less it contributes to the new pixel value. The PSNR scores for the box filter and the gaussian filter are quite similar. The qualitative difference is that the box filtered images seem a bit more blurred than the gaussian images, which is to be expected considering the nature of the filters.YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELQH1DgffAfX"
   },
   "source": [
    "## 4.3 Edge detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtpvRJaQfHN6"
   },
   "source": [
    "Edges appear when there is a sharp change in brightness. In an image this usually corresponds to the boundaries of an object. Edge detection is a fundamental task used in many computer vision applications. One of them is road detection in autonomous driving, which is used for determining the vehicle trajectory.\n",
    "\n",
    "Many different techniques exist for computing the edges. In this section, we will focus on filters that extract the gradient of the image. We will try to detect the road in an still image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpC_NGfufN3R"
   },
   "source": [
    "### 4.3.1 First-order derivative filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8umfWKDIfZD3"
   },
   "source": [
    "**Sobel** kernels approximate the first derivative of a Gaussian filter. Below are the Sobel kernels used in the $x$ and $y$ directions.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{rl}\n",
    "G_x = \\begin{bmatrix} +1 & 0 & -1 \\\\ +2 & 0 & -2 \\\\ +1 & 0 & -1 \\end{bmatrix} * \\mathbf{I}\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{rl}\n",
    "G_y = \\begin{bmatrix} +1 & +2 & +1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{bmatrix} * \\mathbf{I}\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "The gradient magnitude is defined as the square root of the sum of the squares of the horizontal ($G_x$) and the vertical ($G_y$) components of the gradient of an image, such that: \n",
    "\\begin{equation}\n",
    "G =\\sqrt {{G_x}^2+{G_y}^2}\n",
    "\\end{equation}\n",
    "The gradient direction is calculated as follows:\n",
    "\\begin{equation}\n",
    "\\theta= \\tan ^{ - 1}{\\frac{G_y}{G_x}}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7bhn7zvh6bh"
   },
   "source": [
    "### Question (10 pts)\n",
    "Using your implemented function **compute_gradient** on image2.jpg, display the following figures:\n",
    "\n",
    "  1. The gradient of the image in the x-direction.\n",
    "\n",
    "  2. The gradient of the image in the y-direction.\n",
    "\n",
    "  3. The gradient magnitude of each pixel.\n",
    "\n",
    "  4. The gradient direction of each pixel.\n",
    "\n",
    "Discuss what kind of information every image conveys.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8Cy-LVfgjg4"
   },
   "source": [
    "The gradient of the image in the x-direction shows the zones of the image where there is a great variation in the horizontal direction, that is, vertical edges. Its gradient in the y-direction shows horizontal edges.\n",
    "\n",
    "The gradient magnitude represent the variation on each pixel respect to those around it, both horizontally and vertically. It is a more accurate representation of all edges, as it takes both components into account. The gradient direction shows how the edge is oriented instead of how sharp the change is, representig the different angles at which the edge can be oriented as different shades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq-ynXBQgkQ3"
   },
   "source": [
    "###Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fx_8_LPDgoeG"
   },
   "source": [
    "Implement **compute_gradient**\n",
    "\n",
    "**Note:** \n",
    "You are not allowed to use the Python built-in functions for computing gradient. But for doing 2D convolution, you can benefit from *scipy.signal.convolve2d* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(image, kernel, kernel_size, RGB = True):\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]\n",
    "    \n",
    "    if RGB == False:\n",
    "        #We add zero padding to the image\n",
    "        padding_size = int((kernel_size - 1)/2)\n",
    "        img = np.zeros([h+2*padding_size, w+2*padding_size])\n",
    "        img[padding_size:-padding_size, padding_size:-padding_size] = image\n",
    "    \n",
    "        #We apply the kernel\n",
    "        resulting_image = np.zeros([h, w])\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                resulting_image[i, j] = np.sum(img[i:i+kernel_size, j:j+kernel_size]*kernel)\n",
    "\n",
    "        return resulting_image\n",
    "    \n",
    "    #We add zero padding to the image\n",
    "    padding_size = int((kernel_size - 1)/2)\n",
    "    img = np.zeros([h+2*padding_size, w+2*padding_size, 3])\n",
    "    img[padding_size:-padding_size, padding_size:-padding_size, :] = image\n",
    "    \n",
    "    #We apply the kernel\n",
    "    resulting_image = np.zeros([h, w, 3])\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            resulting_image[i, j, 0] = np.sum(img[i:i+kernel_size, j:j+kernel_size, 0]*kernel)\n",
    "            resulting_image[i, j, 1] = np.sum(img[i:i+kernel_size, j:j+kernel_size, 1]*kernel)\n",
    "            resulting_image[i, j, 2] = np.sum(img[i:i+kernel_size, j:j+kernel_size, 2]*kernel)\n",
    "\n",
    "    return resulting_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrmc2tRlgtcp"
   },
   "outputs": [],
   "source": [
    "def compute_gradient(image):\n",
    "    \n",
    "    h, w, aux = image.shape\n",
    "    \n",
    "    GyKernel = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "    GxKernel = GyKernel.T\n",
    "    \n",
    "    Gx = convolution(image/255, GxKernel, 3)\n",
    "    Gy = convolution(image/255, GyKernel, 3)\n",
    "\n",
    "    #Compute the gradient magnitude\n",
    "    im_magnitude = np.sqrt(Gx**2 + Gy**2)\n",
    "    \n",
    "    #Compute the gradient direction\n",
    "    #We cannot divide by 0\n",
    "    Gx[Gx == 0] = 1e-5\n",
    "    im_direction = np.arctan(Gy/Gx)\n",
    "\n",
    "\n",
    "    return Gx, Gy, im_magnitude, im_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = './sample_data/sample_data/image2.jpg'\n",
    "img2 = cv2.imread(img_path)\n",
    "plt.figure()\n",
    "plt.imshow(img2)\n",
    "plt.title(\"Original image\")\n",
    "\n",
    "Gx, Gy, im_magnitude, im_direction = compute_gradient(img2)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace= 0)\n",
    "\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax1.imshow(Gx)\n",
    "ax1.set_title(\"Gradient on the x direction\")\n",
    "\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax2.imshow(Gy)\n",
    "ax2.set_title(\"Gradient on the y direction\")\n",
    "\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "ax3.imshow(im_magnitude)\n",
    "ax3.set_title(\"Gradient magnitude\")\n",
    "\n",
    "ax3 = fig.add_subplot(2, 2, 4)\n",
    "ax3.imshow(im_direction)\n",
    "ax3.set_title(\"Gradient direction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWWN7NEzi1Ly"
   },
   "source": [
    "### 4.3.2 Second-order derivative filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rZSE3toi6iP"
   },
   "source": [
    "Compared to the Sobel filter, a Laplacian of Gaussian (LoG) relies on the second derivative of a Gaussian filter. Hence, it will focus on large gradients in the image. A LoG can be computed by the following three methods:\n",
    "\n",
    "\n",
    "\n",
    "*   method 1: Smoothing the image with a Gaussian kernel (kernel size of 5 and standard deviation of 0.5), then taking the Laplacian of the smoothed image (i.e. second derivative).\n",
    "*   method 2: Convolving the image directly with a LoG kernel (kernel size of 5 and standard deviation of 0.5).\n",
    "\n",
    "*   method 3: Taking the Difference of two Gaussians (DoG) computed at different scales $\\sigma_1$ and $\\sigma_2$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFkZWEA7EmZB"
   },
   "source": [
    "###Exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5hqL-iUEnTz"
   },
   "source": [
    "Implement **compute_LoG**\n",
    "\n",
    "The function should be able to apply any of the above mentioned methods depending on the value passed to the parameter *LOG_type*\n",
    "\n",
    "**Note:** \n",
    "You are not allowed to use the Python built-in functions for computing LOG kernels. But for doing 2D convolution, you can benefit from *scipy.signal.convolve2d* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oOpjcpnFDKY"
   },
   "outputs": [],
   "source": [
    "def compute_LoG(image, LOG_type):\n",
    "    \n",
    "    if LOG_type == 1:\n",
    "        #We first apply the Gaussian kernel\n",
    "        imOut = convolution(image, gauss2D(0.5, 0.5, 5), 5)\n",
    "                \n",
    "        #We then take the laplacian of the resulting image\n",
    "        laplacian_kernel = [[0, 1, 0], [1, -4, 1], [0, 1, 0]]\n",
    "        imOut = convolution(imOut, laplacian_kernel, 3)\n",
    "        \n",
    "    elif LOG_type == 2:\n",
    "        #First we create the LoG kernel\n",
    "        laplacian_kernel = [[0, 1, 0], [1, -4, 1], [0, 1, 0]]\n",
    "        kernel = convolution(gauss2D(0.5, 0.5, 5), laplacian_kernel, 3, RGB = False)\n",
    "        \n",
    "        #We then apply this new filter\n",
    "        imOut = convolution(image, kernel, 5)\n",
    "    \n",
    "    elif LOG_type == 3:\n",
    "        #We set the scales sigma1 and sigma2\n",
    "        sigma1 = 1\n",
    "        sigma2 = 0.5\n",
    "        \n",
    "        imOut = convolution(image, gauss2D(sigma1, sigma1, 5) - gauss2D(sigma2, sigma2, 5), 5)\n",
    "        \n",
    "    return imOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5GSV9v2FMHA"
   },
   "source": [
    "### Questions (10 pts)\n",
    "\n",
    "1.   Test your function using image2.jpg and visualize your results using the three methods.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnXZnTGqqnVw"
   },
   "outputs": [],
   "source": [
    "img_path = './sample_data/sample_data/image2.jpg'\n",
    "img2 = cv2.imread(img_path)\n",
    "\n",
    "result1 = compute_LoG(img2, 1)\n",
    "result2 = compute_LoG(img2, 2)\n",
    "result3 = compute_LoG(img2, 3)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace= 0)\n",
    "\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax1.imshow(img2)\n",
    "ax1.set_title(\"Original image\")\n",
    "\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax2.imshow(result1)\n",
    "ax2.set_title(\"Method 1\")\n",
    "\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "ax3.imshow(result2)\n",
    "ax3.set_title(\"Method 2\")\n",
    "\n",
    "ax3 = fig.add_subplot(2, 2, 4)\n",
    "ax3.imshow(result3)\n",
    "ax3.set_title(\"Method 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DF4d8ZCgPgd"
   },
   "source": [
    "\n",
    "2.   Discuss the difference between applying the three methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1kbBulkgWsn"
   },
   "source": [
    "Method 1 and 2 should be equivalent, as they both consist con convolving a gaussian kernel, a laplacian kernel and an image, with the only difference being the order in which we apply the convolutions. The third method gives us a result wichi is similar but not exactly the same, because we have used a kernel consisting on the difference of two gaussians to approximate the laplacian of a gaussian, instead of convolving a gaussian kernel and a laplacian kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDOR_Ee_gQy5"
   },
   "source": [
    "\n",
    "3.   In the first method, why is it important to convolve an image with a Gaussian before convolving with a Laplacian?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xfas5xWPgXSb"
   },
   "source": [
    "Because we want to apply a kernel corresponding to the Laplacian of a Gaussian, so it is important that the Laplacian kernel is convolved with the Gaussian kernel or, given the associativa property of convolution, to an image already convolved with the Gaussian kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mONfgLCbgR-o"
   },
   "source": [
    "4.   In the third method, what is the best ratio between $\\sigma_1$ and $\\sigma_2$ to achieve the best approximation of the LoG? What is the purpose of having 2 standard deviations?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzCa85MtgYDM"
   },
   "source": [
    "The best ratio would be for $\\sigma_1$ to be the same as in the other two methods, and for $\\sigma_2$ to be slightly smaller, to have the best possible approximation of the LoG. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p577tWgNgTFe"
   },
   "source": [
    "5.   What else is needed to improve the performance and isolate the road,  i.e. what else should be done? You don't have to provide any specific parameter or specific algorithm. Try to propose a direction which would be interesting to explore and how you would approach it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mpHhL9BFlq4"
   },
   "source": [
    "It would be interesting to explore how we can change the values of $\\sigma$ to isolate different parts of the image. For example, a larger $\\sigma$ would pick up less details in the image, so it could be useful for example the road, while a smaller $\\sigma$ would give us a more detailed representation of the leaves on the trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sgm7JVhFubg"
   },
   "source": [
    "## 4.4 Foreground-background separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewTmbkTFHh01"
   },
   "source": [
    "Foreground-background separation is an important task in the field of computer vision (see Figure). In this exercise, you will implement a simple unsupervised algorithm that leverages the variations in texture to segment the foreground object from the background. We will assume the foreground object has a distinct combination of textures compared to background. As mentioned earlier, Gabor filters are well-suited for texture analysis thanks to their frequency domain characteristics. Therefore, we will use a collection of Gabor filters with varying scale and orientations which we call a *filter bank*. The outline of the algorithm is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UScTstnoH8Yz"
   },
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1w60xJ4UlG60Ie6ljRkHn0GJDdVu9eb5e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieFPLOKTImzD"
   },
   "source": [
    "**(Left)** Input image, **(Middle)** Foreground mask, **(Right)** Masked object. Foreground-Background separation aims at masking out the salient object pixels from the background pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cggieAGXBNp"
   },
   "source": [
    "---\n",
    "\n",
    "**Algorithm 1** Foreground-Background Segmentation Algorithm\n",
    "\n",
    "---\n",
    "\n",
    "**Input:** $x$ - input image\n",
    "\n",
    "**Output:** $y$ - pixelwise labels\n",
    "\n",
    "\n",
    "\n",
    "1.   Convert to grayscale if necessary.\n",
    "\n",
    ">>**if** $x$ is RGB **then**\n",
    "\n",
    ">>>$x$ $\\leftarrow$ rgb2gray($x$)\n",
    "\n",
    ">>**end if**\n",
    "\n",
    "2.   Create Gabor filterbank, $\\mathcal{F}_{gabor}$, with varying $\\sigma$, $\\lambda$ and $\\theta$.\n",
    "\n",
    "3.   Filter $x$ with the filterbank. Store each output in $fmaps$.\n",
    "\n",
    "4.   Compute the magnitude of the complex $fmaps$. Store the results in $fmags$.\n",
    "\n",
    ">>$fmags$ $\\leftarrow$  $\\vert fmaps \\vert$\n",
    "\n",
    "5.   Smooth $fmags$.\n",
    "\n",
    ">>$fmags$ $\\leftarrow$  smooth($fmags$)\n",
    "\n",
    "6.   Convert $fmags$ into data matrix, $f$.\n",
    "\n",
    ">>$f$ $\\leftarrow$  reshape($fmags$)\n",
    "\n",
    "7.   Cluster $f$ using kmeans into two sets.\n",
    "\n",
    ">>$y$ $\\leftarrow$  kmeans($f$, 2)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo-iPmhla2G9"
   },
   "source": [
    "### Questions (20 pts)\n",
    "\n",
    "1.   Run the algorithm on all test images with the provided parameter settings. What do you observe? Explain shortly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwYMD8qkdx5m"
   },
   "source": [
    "We can observe that when using the default parameters the algorithm tries to separate the foreground from the background, but fails to do so correctly. Sometimes only a part of the foreground is separated, and the rest stays with the background, and sometimes parts of the background are considered foreground and separated from the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fG0G4XmldzDB"
   },
   "source": [
    "\n",
    "2.   Experiment with different $\\lambda$, $\\sigma$ and $\\theta$ settings until you get reasonable outputs. Report what parameter settings work better for each input image and try to explain why.\n",
    "\n",
    ">**Hint:** \n",
    "Don't change multiple variables at once. You might not need to change some at all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMcXMskXdzFO"
   },
   "source": [
    "In general, we did not need to change the values of theta, as they already covered the whole angle spectrum with a reasonable interval.\n",
    "\n",
    "For $\\lambda$, we have found that the best results were achieved when it remained fixed as $\\lambda = lambdaMin$, as this allows us to find sharper textures. The exception being the image \"cows\", where all textures were very uniform.\n",
    "\n",
    "In respect to $\\sigma$, we have found that for some images, such as the robins, very small values ($\\sigma$ < 0.5) work best, while slightly greater values (0.5 < $\\sigma$ < 1) work better for the polar bear and bigger values ($\\sigma$ > 5) for the rest. This may be due to the need to detect very intricate textures in the feathers of the robins and the grass around the polar bear, whereas in the other images the textures are much less detailed. The exception is the image 'Kobi', where values of $\\sigma$ similar to those of the polar bear seem to work best, but cannot separate the dog from the floor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NzeqBhJdzHv"
   },
   "source": [
    "3.   After you achieve good separation on all test images, run the script again with corresponding parameters but this time with\n",
    "\n",
    ">>>smoothingFlag = False\n",
    "\n",
    ">Describe what you observe at the output when smoothing is not applied on the magnitude images. Explain why it happens and try to reason about the motivation behind this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCyG914YcKcb"
   },
   "source": [
    "When smoothing is not applied, the separation between the foreground and the background becomes more detailed, that is, it tries to better follow the shape of the object. However, it si often mistaken by small details, and thus the results are worse that in the case where smoothing was applied. We apply this filter to prevent this from happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiDCgYrlcL9r"
   },
   "source": [
    "###Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mR5GreUHcPtd"
   },
   "source": [
    "Please get yourself familiar with provided skeleton code **gabor_segmentation**.py. Keep in mind that you will need your implementation of the **createGabor** function.\n",
    "\n",
    "When you succesfully implement it all, it should run without problems and produce a reasonable segmentation with the default parameters on **kobi.png**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3f9_GNhvqH7"
   },
   "outputs": [],
   "source": [
    "image_id = \"Polar\"\n",
    "img = load_image(image_id)\n",
    "show_image(img, f'Input image: {image_id}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abk-us2ZiR3k"
   },
   "outputs": [],
   "source": [
    "# Control settings\n",
    "visFlag       = False   #  Set to true to visualize filter responses.\n",
    "smoothingFlag = True   #  Set to true to postprocess filter outputs.\n",
    "\n",
    "# Design array of Gabor Filters\n",
    "\n",
    "numRows, numCols = img.shape\n",
    "\n",
    "# Estimate the minimum and maximum of the wavelengths for the sinusoidal\n",
    "# carriers. \n",
    "# ** This step is pretty much standard, therefore, you don't have to\n",
    "#    worry about it. It is cycles in pixels. ** \n",
    "lambdaMin = 4/np.sqrt(2)\n",
    "lambdaMax = np.sqrt(abs(numRows)**2 + abs(numCols)**2)\n",
    "\n",
    "# Specify the carrier wavelengths.  \n",
    "# (or the central frequency of the carrier signal, which is 1/lambda)\n",
    "n = np.floor(np.log2(lambdaMax/lambdaMin))\n",
    "dTheta       = 2 * np.pi/8                  # \\\\ the step size\n",
    "\n",
    "if image_id == 'Kobi':\n",
    "    lambdas = 2**np.arange(0, 1) * lambdaMin\n",
    "    sigmas = np.arange(0.2, 1, 0.2)\n",
    "    orientations = np.arange(0, np.pi+dTheta, dTheta)          \n",
    "elif image_id == 'Polar':\n",
    "    lambdas = 2**np.arange(0, 1) * lambdaMin\n",
    "    sigmas = np.arange(0.5, 1, 0.1)\n",
    "    orientations = np.arange(0, np.pi+dTheta, dTheta)          \n",
    "elif image_id == 'Robin-1':\n",
    "    lambdas = 2**np.arange(0, 1) * lambdaMin\n",
    "    sigmas = np.arange(0.22, 0.5, 0.05)\n",
    "    orientations = np.arange(0, np.pi+dTheta, dTheta)  \n",
    "elif image_id =='Robin-2':\n",
    "    lambdas = 2**np.arange(0, 1) * lambdaMin\n",
    "    sigmas = np.arange(0.22, 0.5, 0.05)\n",
    "    orientations = np.arange(0, np.pi+dTheta, dTheta)      \n",
    "elif image_id == 'Cows':\n",
    "    lambdas = 2**np.arange((n-2), (n-2)+1, (n-2)/4) * lambdaMin\n",
    "    sigmas = np.arange(10, 15, 1)\n",
    "    orientations = np.arange(0, np.pi+dTheta, dTheta)        \n",
    "elif image_id == 'SciencePark':\n",
    "    lambdas = 2**np.arange(0, (n-2)+1) * lambdaMin\n",
    "    sigmas = np.arange([1,2])\n",
    "    orientations = np.arange(0, np.pi+dTheta, dTheta*2)          \n",
    "else:\n",
    "    lambdas = 2**np.arange(0, (n-2)+1) * lambdaMin\n",
    "    sigmas = np.array([1,2])\n",
    "    orientations = np.arange(0, np.pi+dTheta, dTheta)               \n",
    "\n",
    "\n",
    "# Now you can create the filterbank.\n",
    "gaborFilterBank = []\n",
    "tic = time.time()\n",
    "for lmbda in lambdas:\n",
    "    for sigma in sigmas:\n",
    "        for theta in orientations:\n",
    "            # Filter parameter configuration for this filter.\n",
    "            psi    = 0\n",
    "            gamma  = 0.5\n",
    "            \n",
    "            # Create a Gabor filter with the specs above. \n",
    "            filter_config = {}\n",
    "            filter_config[\"filterPairs\"] = createGabor( sigma, theta, lmbda, psi, gamma )\n",
    "            filter_config[\"sigma\"]       = sigma\n",
    "            filter_config[\"lmbda\"]       = lmbda\n",
    "            filter_config[\"theta\"]       = theta\n",
    "            filter_config[\"psi\"]         = psi\n",
    "            filter_config[\"gamma\"]       = gamma\n",
    "            gaborFilterBank.append(filter_config)\n",
    "ctime = time.time() - tic\n",
    "\n",
    "print('--------------------------------------\\n \\t\\tDetails\\n--------------------------------------')\n",
    "print(f'Total number of filters       : {len(gaborFilterBank)}')\n",
    "print(f'Number of scales (sigma)      : {len(sigmas)}')\n",
    "print(f'Number of orientations (theta): {len(orientations)}')\n",
    "print(f'Number of carriers (lambda)   : {len(lambdas)}')\n",
    "print(f'---------------------------------------')\n",
    "print(f'Filter bank created in {ctime} seconds.')\n",
    "print(f'---------------------------------------')\n",
    "\n",
    "# Filter images using Gabor filter bank using quadrature pairs (real and imaginary parts)\n",
    "featureMaps = []\n",
    "\n",
    "for gaborFilter in gaborFilterBank:\n",
    "    real_out = cv2.filter2D(img, -1 , gaborFilter[\"filterPairs\"][:, :, 0], borderType=cv2.BORDER_CONSTANT)  # \\\\TODO: filter the grayscale input with real part of the Gabor\n",
    "    imag_out = cv2.filter2D(img, -1 , gaborFilter[\"filterPairs\"][:, :, 1], borderType=cv2.BORDER_CONSTANT)  # \\\\TODO: filter the grayscale input with imaginary part of the Gabor\n",
    "    featureMaps.append(np.stack((real_out, imag_out), 2))\n",
    "\n",
    "    # Visualize the filter responses if you wish.\n",
    "    if visFlag:\n",
    "        fig = plt.figure()\n",
    "\n",
    "        ax = fig.add_subplot(1, 2, 1)\n",
    "        ax.imshow(real_out)    # Real\n",
    "        title = \"Re[h(x,y)], \\n lambda = {0:.4f}, \\n theta = {1:.4f}, \\n sigma = {2:.4f}\".format(gaborFilter[\"lmbda\"], gaborFilter[\"theta\"], gaborFilter[\"sigma\"])\n",
    "        ax.set_title(title)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        ax = fig.add_subplot(1, 2, 2)\n",
    "        ax.imshow(imag_out)    # Real\n",
    "        title = \"Im[h(x,y)], \\n lambda = {0:.4f}, \\n theta = {1:.4f}, \\n sigma = {2:.4f}\".format(gaborFilter[\"lmbda\"], gaborFilter[\"theta\"], gaborFilter[\"sigma\"])\n",
    "        ax.set_title(title)\n",
    "        ax.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "# Compute the magnitude\n",
    "featureMags = []\n",
    "for i, fm in enumerate(featureMaps):\n",
    "    real_part = fm[...,0]\n",
    "    imag_part = fm[...,1]\n",
    "    mag = np.sqrt(np.power(real_part, 2) + np.power(imag_part, 2))  # \\\\TODO: Compute the magnitude here\n",
    "    featureMags.append(mag)\n",
    "    \n",
    "    # Visualize the magnitude response if you wish.\n",
    "    if visFlag:\n",
    "        fig = plt.figure()\n",
    "\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.imshow(mag.astype(np.uint8))    # visualize magnitude\n",
    "        title = \"Re[h(x,y)], \\n lambda = {0:.4f}, \\n theta = {1:.4f}, \\n sigma = {2:.4f}\".format(gaborFilterBank[i][\"lmbda\"], \n",
    "                                                                                                 gaborFilterBank[i][\"theta\"], \n",
    "                                                                                                 gaborFilterBank[i][\"sigma\"])\n",
    "        ax.set_title(title)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "# Prepare and Preprocess features   \n",
    "features = np.zeros(shape=(numRows, numCols, len(featureMags)))\n",
    "if smoothingFlag:\n",
    "    for i in [1, len(featureMags)]:\n",
    "        features[:, :, i-1] = convolution(featureMags[i-1], gauss2D(3, 3, 5), 5, RGB=False)\n",
    "else:\n",
    "    # Don't smooth but just insert magnitude images into the matrix\n",
    "    # called features.\n",
    "    for i, fmag in enumerate(featureMags):\n",
    "        features[:,:,i] = fmag\n",
    "\n",
    "\n",
    "# Reshape the filter outputs (i.e. tensor called features) of size \n",
    "# [numRows, numCols, numFilters] into a matrix of size [numRows*numCols, numFilters]\n",
    "# This will constitute our data matrix which represents each pixel in the \n",
    "# input image with numFilters features.  \n",
    "features = np.reshape(features, newshape=(numRows * numCols, -1))\n",
    "\n",
    "\n",
    "# Standardize features. \n",
    "features_standarized = (features - np.mean(features))/np.std(features)\n",
    "\n",
    "# (Optional) Visualize the saliency map using the first principal component \n",
    "# of the features matrix. It will be useful to diagnose possible problems \n",
    "# with the pipeline and filterbank.\n",
    "transformed_feature = PCA(n_components=1).fit_transform(features_standarized) # select the first component\n",
    "transformed_feature = np.ascontiguousarray(transformed_feature, dtype=np.float32)\n",
    "feature2DImage = np.reshape(transformed_feature,newshape=(numRows,numCols))\n",
    "plt.figure()\n",
    "plt.title(f'Pixel representation projected onto first PC')\n",
    "plt.imshow(feature2DImage, cmap='gray')\n",
    "plt.axis(\"off\") \n",
    "plt.show()\n",
    "\n",
    "# Apply k-means algorithm to cluster pixels using the data matrix,\n",
    "# features. \n",
    "import sklearn.cluster\n",
    "from sklearn.cluster import KMeans\n",
    "tic = time.time()\n",
    "aux = sklearn.cluster.KMeans(2)# \\\\TODO: Return cluster labels per pixel\n",
    "pixLabels = aux.fit(transformed_feature).labels_\n",
    "print(type(pixLabels))\n",
    "print(pixLabels)\n",
    "ctime = time.time() - tic\n",
    "print(f'Clustering completed in {ctime} seconds.')\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the clustering by reshaping pixLabels into original grayscale\n",
    "# input size [numRows numCols].\n",
    "pixLabels = np.reshape(pixLabels, newshape=(numRows, numCols))\n",
    "plt.figure()\n",
    "plt.title(f'Pixel clusters')\n",
    "plt.imshow(pixLabels)\n",
    "plt.axis(\"off\") \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Use the pixLabels to visualize segmentation.\n",
    "Aseg1 = np.zeros_like(img)\n",
    "Aseg2 = np.zeros_like(img)\n",
    "BW = pixLabels == 1  # check for the value of your labels in pixLabels (could be 1 or 0 instead of 2)\n",
    "#BW = np.repeat(BW[:, :, np.newaxis], 3, axis=2) # do this only if you have 3 channels in the img\n",
    "Aseg1[BW] = img[BW]\n",
    "Aseg2[~BW] = img[~BW]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f'montage')\n",
    "plt.imshow(Aseg1, 'gray', interpolation='none')\n",
    "plt.imshow(Aseg2, 'jet',  interpolation='none', alpha=0.7)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#Plot\n",
    "fit, ax = plt.subplots(1, 2, figsize=(15, 15))\n",
    "ax[0].imshow(Aseg1, cmap = 'gray')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(Aseg2, cmap = 'gray')\n",
    "ax[1].axis('off')\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1oBHyce1saAI",
    "V2NtMMr9RSVv",
    "RkT7BklaTUmj",
    "762bvpRYTual",
    "-2ZB6tEeV_Ls",
    "ViVNlHPOa2Jd",
    "eMgRkSaSa-7u",
    "H1mP6EytbZE2",
    "a2fL29EAbaYt",
    "32eQTHQobdGG",
    "WIpIVPE4tvE9",
    "38iQckSqkLjC"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
